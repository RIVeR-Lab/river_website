@inproceedings{sharif_end--end_2021,
 abstract = {State-of-the-art human-in-the-loop robot grasping is hugely suffered by Electromyography (EMG) inference robustness issues. As a workaround, researchers have been looking into integrating EMG with other signals, often in an ad hoc manner. In this paper, we are presenting a method for end-to-end training of a policy for human-in-the-loop robot grasping on real reaching trajectories. For this purpose we use Reinforcement Learning (RL) and Imitation Learning (IL) in DEXTRON (DEXTerity enviRONment), a stochastic simulation environment with real human trajectories that are augmented and selected using a Monte Carlo (MC) simulation method. We also offer a success model which once trained on the expert policy data and the RL policy roll-out transitions, can provide transparency to how the deep policy works and when it is probably going to fail.},
 author = {Sharif, Mohammadreza and Erdogmus, Deniz and Amato, Christopher and Padir, Taskin},
 booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/ICRA48506.2021.9561937},
 file = {IEEE Xplore Full Text PDF:/home/adrianselva/Zotero/storage/6QKDB46N/Sharif et al. - 2021 - End-to-end grasping policies for human-in-the-loop.pdf:application/pdf},
 keywords = {Electromyography, Grasping, Monte Carlo methods, Reinforcement learning, Robustness, Stochastic processes, Training},
 month = {May},
 note = {ISSN: 2577-087X},
 pages = {2768--2774},
 title = {End-to-end grasping policies for human-in-the-loop robots via deep reinforcement learning},
 year = {2021}
}

